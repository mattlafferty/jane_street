{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgbm\n",
    "import optuna as opt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gc\n",
    "import shap\n",
    "from scipy.stats import binom_test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewlafferty/Dropbox/Kaggle/Jane_Street/jane-street-market-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491892</th>\n",
       "      <td>80</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>-0.014573</td>\n",
       "      <td>1</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>-0.788714</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.344326</td>\n",
       "      <td>2.704979</td>\n",
       "      <td>-2.592726</td>\n",
       "      <td>3.000305</td>\n",
       "      <td>-3.388496</td>\n",
       "      <td>3.263304</td>\n",
       "      <td>-2.804504</td>\n",
       "      <td>3.027151</td>\n",
       "      <td>-2.440436</td>\n",
       "      <td>491892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219626</th>\n",
       "      <td>34</td>\n",
       "      <td>0.285587</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.015364</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491193</td>\n",
       "      <td>0.147054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090701</td>\n",
       "      <td>1.782374</td>\n",
       "      <td>0.395661</td>\n",
       "      <td>1.424740</td>\n",
       "      <td>0.410524</td>\n",
       "      <td>2.460661</td>\n",
       "      <td>0.678450</td>\n",
       "      <td>2.277390</td>\n",
       "      <td>0.635024</td>\n",
       "      <td>219626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015110</th>\n",
       "      <td>429</td>\n",
       "      <td>3.146723</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.001278</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.004575</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751577</td>\n",
       "      <td>3.397723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744954</td>\n",
       "      <td>0.234358</td>\n",
       "      <td>2.206868</td>\n",
       "      <td>-0.452832</td>\n",
       "      <td>0.602040</td>\n",
       "      <td>-0.132379</td>\n",
       "      <td>1.528540</td>\n",
       "      <td>0.203499</td>\n",
       "      <td>1.997672</td>\n",
       "      <td>2015110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897312</th>\n",
       "      <td>184</td>\n",
       "      <td>0.566679</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.017521</td>\n",
       "      <td>-0.033944</td>\n",
       "      <td>-0.019181</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.416806</td>\n",
       "      <td>-0.403020</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.112464</td>\n",
       "      <td>-1.288962</td>\n",
       "      <td>-2.222993</td>\n",
       "      <td>-0.995549</td>\n",
       "      <td>-3.221498</td>\n",
       "      <td>-1.500098</td>\n",
       "      <td>-2.159030</td>\n",
       "      <td>-1.432691</td>\n",
       "      <td>-2.029860</td>\n",
       "      <td>897312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198769</th>\n",
       "      <td>255</td>\n",
       "      <td>2.800629</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.024165</td>\n",
       "      <td>0.045762</td>\n",
       "      <td>0.033423</td>\n",
       "      <td>1</td>\n",
       "      <td>2.093228</td>\n",
       "      <td>0.089239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360148</td>\n",
       "      <td>1.250625</td>\n",
       "      <td>-0.433197</td>\n",
       "      <td>1.347622</td>\n",
       "      <td>-0.055718</td>\n",
       "      <td>1.910187</td>\n",
       "      <td>-0.133741</td>\n",
       "      <td>1.682871</td>\n",
       "      <td>-0.170468</td>\n",
       "      <td>1198769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455783</th>\n",
       "      <td>72</td>\n",
       "      <td>0.334733</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>-0.014786</td>\n",
       "      <td>-0.019530</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554481</td>\n",
       "      <td>0.870773</td>\n",
       "      <td>-0.790721</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>-2.258830</td>\n",
       "      <td>0.708828</td>\n",
       "      <td>-1.270733</td>\n",
       "      <td>0.877817</td>\n",
       "      <td>-0.874588</td>\n",
       "      <td>455783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337811</th>\n",
       "      <td>284</td>\n",
       "      <td>0.890416</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.043398</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.093788</td>\n",
       "      <td>0.817239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.476501</td>\n",
       "      <td>0.331179</td>\n",
       "      <td>0.251048</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.229638</td>\n",
       "      <td>-0.167930</td>\n",
       "      <td>0.451875</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>1337811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311783</th>\n",
       "      <td>486</td>\n",
       "      <td>0.077688</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>5.297500</td>\n",
       "      <td>-1.530664</td>\n",
       "      <td>1.852107</td>\n",
       "      <td>-0.688117</td>\n",
       "      <td>5.537706</td>\n",
       "      <td>-1.193737</td>\n",
       "      <td>4.159283</td>\n",
       "      <td>-1.201244</td>\n",
       "      <td>3.337126</td>\n",
       "      <td>2311783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339810</th>\n",
       "      <td>285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002936</td>\n",
       "      <td>-0.002981</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.014357</td>\n",
       "      <td>3.461213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.874482</td>\n",
       "      <td>-0.048451</td>\n",
       "      <td>1.356119</td>\n",
       "      <td>-0.625356</td>\n",
       "      <td>1.936857</td>\n",
       "      <td>-0.516289</td>\n",
       "      <td>1.817743</td>\n",
       "      <td>-0.378154</td>\n",
       "      <td>1339810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535660</th>\n",
       "      <td>87</td>\n",
       "      <td>1.416155</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.001050</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.276139</td>\n",
       "      <td>-1.557572</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.480704</td>\n",
       "      <td>-2.161251</td>\n",
       "      <td>0.272887</td>\n",
       "      <td>-2.299898</td>\n",
       "      <td>-1.777989</td>\n",
       "      <td>-4.166561</td>\n",
       "      <td>-1.227035</td>\n",
       "      <td>-2.936782</td>\n",
       "      <td>-0.312190</td>\n",
       "      <td>535660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "491892     80  0.059207  0.003374  0.007049  0.010831 -0.012751 -0.014573   \n",
       "219626     34  0.285587  0.002744  0.004567  0.007870  0.015364  0.018541   \n",
       "2015110   429  3.146723 -0.000063 -0.001278 -0.002908 -0.004575 -0.002998   \n",
       "897312    184  0.566679  0.001359 -0.000410 -0.017521 -0.033944 -0.019181   \n",
       "1198769   255  2.800629  0.001755  0.008786  0.024165  0.045762  0.033423   \n",
       "455783     72  0.334733  0.001044  0.000166  0.002704 -0.014786 -0.019530   \n",
       "1337811   284  0.890416  0.004181  0.009401  0.027605  0.043398  0.030062   \n",
       "2311783   486  0.077688 -0.000734  0.000584  0.004784  0.009934  0.007780   \n",
       "1339810   285  0.000000 -0.002936 -0.002981  0.001015  0.010478  0.009534   \n",
       "535660     87  1.416155 -0.000310 -0.001555 -0.001050 -0.004225 -0.005818   \n",
       "\n",
       "         feature_0  feature_1  feature_2  ...  feature_121  feature_122  \\\n",
       "491892           1   1.107300  -0.788714  ...    -3.344326     2.704979   \n",
       "219626           1   0.491193   0.147054  ...    -0.090701     1.782374   \n",
       "2015110          1   0.751577   3.397723  ...     0.744954     0.234358   \n",
       "897312           1  -0.416806  -0.403020  ...    -3.112464    -1.288962   \n",
       "1198769          1   2.093228   0.089239  ...    -0.360148     1.250625   \n",
       "455783           1  -3.172026  -3.093182  ...    -0.554481     0.870773   \n",
       "1337811         -1   1.093788   0.817239  ...     0.055214     0.476501   \n",
       "2311783         -1  -3.172026  -3.093182  ...     5.297500    -1.530664   \n",
       "1339810         -1   2.014357   3.461213  ...          NaN     1.874482   \n",
       "535660          -1  -2.276139  -1.557572  ...    -1.480704    -2.161251   \n",
       "\n",
       "         feature_123  feature_124  feature_125  feature_126  feature_127  \\\n",
       "491892     -2.592726     3.000305    -3.388496     3.263304    -2.804504   \n",
       "219626      0.395661     1.424740     0.410524     2.460661     0.678450   \n",
       "2015110     2.206868    -0.452832     0.602040    -0.132379     1.528540   \n",
       "897312     -2.222993    -0.995549    -3.221498    -1.500098    -2.159030   \n",
       "1198769    -0.433197     1.347622    -0.055718     1.910187    -0.133741   \n",
       "455783     -0.790721     0.264029    -2.258830     0.708828    -1.270733   \n",
       "1337811     0.331179     0.251048     0.184762     0.229638    -0.167930   \n",
       "2311783     1.852107    -0.688117     5.537706    -1.193737     4.159283   \n",
       "1339810    -0.048451     1.356119    -0.625356     1.936857    -0.516289   \n",
       "535660      0.272887    -2.299898    -1.777989    -4.166561    -1.227035   \n",
       "\n",
       "         feature_128  feature_129    ts_id  \n",
       "491892      3.027151    -2.440436   491892  \n",
       "219626      2.277390     0.635024   219626  \n",
       "2015110     0.203499     1.997672  2015110  \n",
       "897312     -1.432691    -2.029860   897312  \n",
       "1198769     1.682871    -0.170468  1198769  \n",
       "455783      0.877817    -0.874588   455783  \n",
       "1337811     0.451875     0.179533  1337811  \n",
       "2311783    -1.201244     3.337126  2311783  \n",
       "1339810     1.817743    -0.378154  1339810  \n",
       "535660     -2.936782    -0.312190   535660  \n",
       "\n",
       "[10 rows x 138 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = 1 * (data['resp'] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(data) if 'feature' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shadow_features(X:pd.DataFrame, features:list) -> pd.DataFrame:  \n",
    "    for feature in features:\n",
    "        temp = X[feature].to_numpy(copy=True)\n",
    "        np.random.shuffle(temp) # shuffle the values of each feature to all the features\n",
    "        X[feature + '_shadow'] = temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_tune(X:pd.DataFrame, target:str, features:list, num_boost_rounds:int, params:dict, nfolds:int=3, verbose_eval:bool = True, sample:float = 0.5) -> dict:\n",
    "    \n",
    "    X = X.sample(frac=sample)\n",
    "    \n",
    "    dtrain = lgbm.Dataset(data=X[features],\n",
    "                          label=X[target],\n",
    "                          feature_name=features)\n",
    "    \n",
    "    model = opt.integration.lightgbm.LightGBMTunerCV(params = params,\n",
    "                                                     train_set = dtrain,\n",
    "                                                     num_boost_round = num_boost_rounds,\n",
    "                                                     nfold = nfolds,\n",
    "                                                     stratified = True,\n",
    "                                                     shuffle = True,\n",
    "                                                     feature_name=features,\n",
    "                                                     early_stopping_rounds=0.05*num_boost_rounds,\n",
    "                                                     verbose_eval = verbose_eval,\n",
    "                                                     seed = 51)\n",
    "    model.run()\n",
    "    print(model.best_score)\n",
    "    return model.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2021-01-10 08:19:50,531] A new study created in memory with name: no-name-9120fd8b-44a7-461c-aa31-6850cc0b32c8\n",
      "\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: -inf:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: 0.537516:   0%|          | 0/7 [00:41<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: 0.537516:  14%|#4        | 1/7 [00:41<04:07, 41.30s/it][I 2021-01-10 08:20:31,838] Trial 0 finished with value: 0.5375160768507232 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.5375160768507232.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.537516:  14%|#4        | 1/7 [00:41<04:07, 41.30s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  14%|#4        | 1/7 [01:45<04:07, 41.30s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  29%|##8       | 2/7 [01:45<04:01, 48.32s/it][I 2021-01-10 08:21:36,527] Trial 1 finished with value: 0.5388134106108794 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.5388134106108794.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  29%|##8       | 2/7 [01:45<04:01, 48.32s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  29%|##8       | 2/7 [04:22<04:01, 48.32s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [04:22<05:23, 80.79s/it][I 2021-01-10 08:24:13,085] Trial 2 finished with value: 0.5429810769415407 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.5429810769415407.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [04:22<05:23, 80.79s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [05:39<05:23, 80.79s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  57%|#####7    | 4/7 [05:39<03:58, 79.50s/it][I 2021-01-10 08:25:29,594] Trial 3 finished with value: 0.5417169069269036 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.5429810769415407.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  57%|#####7    | 4/7 [05:39<03:58, 79.50s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  57%|#####7    | 4/7 [11:20<03:58, 79.50s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [11:20<05:16, 158.19s/it][I 2021-01-10 08:31:11,399] Trial 4 finished with value: 0.5464531535040185 and parameters: {'feature_fraction': 0.7}. Best is trial 4 with value: 0.5464531535040185.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [11:20<05:16, 158.19s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [12:05<05:16, 158.19s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:05<02:04, 124.09s/it][I 2021-01-10 08:31:55,916] Trial 5 finished with value: 0.5398975385133017 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 4 with value: 0.5464531535040185.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:05<02:04, 124.09s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:36<02:04, 124.09s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453: 100%|##########| 7/7 [12:36<00:00, 96.10s/it] [I 2021-01-10 08:32:26,687] Trial 6 finished with value: 0.5377752471851088 and parameters: {'feature_fraction': 0.5}. Best is trial 4 with value: 0.5464531535040185.\n",
      "feature_fraction, val_score: 0.546453: 100%|##########| 7/7 [12:36<00:00, 108.02s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.546453:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   0%|          | 0/20 [12:04<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [12:04<3:49:19, 724.20s/it][I 2021-01-10 08:44:30,896] Trial 7 finished with value: 0.5589578208690908 and parameters: {'num_leaves': 143}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [12:04<3:49:19, 724.20s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [19:25<3:49:19, 724.20s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  10%|#         | 2/20 [19:25<3:11:48, 639.39s/it][I 2021-01-10 08:51:52,391] Trial 8 finished with value: 0.5512564187373068 and parameters: {'num_leaves': 64}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  10%|#         | 2/20 [19:25<3:11:48, 639.39s/it]"
     ]
    }
   ],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'verbosity': -1,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'learning_rate': 0.25,\n",
    "          'is_unbalanced': False,\n",
    "          'metric': 'auc'}\n",
    "\n",
    "params_0 = params_tune(X=data,\n",
    "                       target='target',\n",
    "                       features=[x for x in features if x not in ['resp','weight']],\n",
    "                       num_boost_rounds=5000,\n",
    "                       params=params,\n",
    "                       nfolds=3,\n",
    "                       verbose_eval=False,\n",
    "                       sample = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'verbosity': -1,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'learning_rate': 0.25,\n",
       " 'is_unbalanced': False,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.4973315278772257,\n",
       " 'lambda_l2': 0.004318282866597431,\n",
       " 'num_leaves': 254,\n",
       " 'feature_fraction': 1.0,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 10}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_params['metric'] = 'auc'\n",
    "test_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgbm.Dataset(data=data[[x for x in features if x not in ['resp','weight']]],\n",
    "                          label=data['target'],\n",
    "                          feature_name=[x for x in features if x not in ['resp','weight']])\n",
    "\n",
    "test_params['metric'] = 'auc'\n",
    "test_model = lgbm.cv(params = test_params,\n",
    "                        train_set = dtrain,\n",
    "                        num_boost_round=10000,\n",
    "                        nfold=5,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        metrics='auc',\n",
    "                        feature_name=[x for x in features if x not in ['resp','weight']],\n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'metric': 'auc',\n",
       " 'verbosity': -1,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'learning_rate': 0.25,\n",
       " 'is_unbalanced': False,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 0.4973315278772257,\n",
       " 'lambda_l2': 0.004318282866597431,\n",
       " 'num_leaves': 254,\n",
       " 'feature_fraction': 1.0,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 10}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_shap(X:pd.DataFrame, features:list, target:str, params:dict, num_boost_rounds:int=10000, sample:float = 0.7, num_iter:int = 3) -> list:\n",
    "    \n",
    "    # We will determine optimal parameters on a random sample. We'll use these parameters to construct a new model for each iteration.\n",
    "    # Determining the hyperparameters takes time, so we'll do it once, and use these parameters to construct our models.\n",
    "    X_shadow = create_shadow_features(X, features)\n",
    "    \n",
    "    # These are the features, categorical features, and categorical feature indices that we will use once the shadow features are added.\n",
    "    shadow_features = features + [x + '_shadow' for x in features]\n",
    "                        \n",
    "    # This is the data frame that we will use to store our SHAP value information\n",
    "    results_df = pd.DataFrame(columns = shadow_features + [target])\n",
    "    \n",
    "    # With the parameters determined, we'll build models using the parameters on a random sample of records, determine the SHAP values on\n",
    "    # the remaining records.\n",
    "    for i in range(num_iter):\n",
    "        X_train, X_shap, y_train, y_shap = train_test_split(X_shadow[shadow_features], X_shadow[target], train_size=sample, shuffle=True, stratify=X_shadow[target])\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data = X_train,\n",
    "                              label = y_train,\n",
    "                              feature_name = shadow_features)\n",
    "        \n",
    "        dval = lgbm.Dataset(data = X_shap,\n",
    "                            label = y_shap,\n",
    "                            feature_name = shadow_features)\n",
    "        \n",
    "        del(X_train, y_train)\n",
    "        gc.collect()\n",
    "        \n",
    "        current_model = lgbm.train(params=params,\n",
    "                                   train_set=dtrain,\n",
    "                                   num_boost_round=num_boost_rounds,\n",
    "                                   valid_sets=dval,\n",
    "                                   feature_name=shadow_features,\n",
    "                                   early_stopping_rounds=int(0.05*num_boost_rounds),\n",
    "                                   verbose_eval=100)\n",
    "        \n",
    "        shap_values = shap.TreeExplainer(current_model).shap_values(X_shap)\n",
    "        shap_values_df = pd.DataFrame(data = shap_values[1], columns = shadow_features)\n",
    "        shap_values_df[target] = np.where(y_shap == 1, 1, 0)\n",
    "        \n",
    "        print('Missing targets in y_shap = ', sum(y_shap.isna()))\n",
    "        print('Missing targets in y_shap = ', sum(y_shap))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target'].isna()))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target']))\n",
    "        \n",
    "        #print(results.shape)\n",
    "        #print(np.abs(shap_values[0]).mean(0))\n",
    "        print(results_df.shape)\n",
    "        #print(results_df.sample(10))\n",
    "        print(shap_values_df.shape)\n",
    "        print([x for x in list(results_df) if x not in list(shap_values_df)])\n",
    "        print([x for x in list(shap_values_df) if x not in list(results_df)])\n",
    "        #print(shap_values_df.sample(10))\n",
    "        results_df = pd.concat([results_df, shap_values_df], axis = 0)\n",
    "    \n",
    "    cols_to_keep = []\n",
    "    n = results_df.shape[0]\n",
    "    for col in features:\n",
    "        x = np.sum(np.where((results_df[target] == 0) & (results_df[col] < results_df[col + '_shadow']), 1,\n",
    "                        np.where((results_df[target] == 1) & (results_df[col] > results_df[col + '_shadow']), 1, 0)))\n",
    "        p_val = binom_test(x=x, n=n, p=0.5, alternative='greater')\n",
    "        if p_val <= 0.05:\n",
    "            cols_to_keep += [col]\n",
    "            \n",
    "    return cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing targets in y_shap =  0\n",
      "Missing targets in y_shap =  361493\n",
      "Missing targets in shap_values_df =  0\n",
      "Missing targets in shap_values_df =  361493\n",
      "(0, 265)\n",
      "(717148, 265)\n",
      "[]\n",
      "[]\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing targets in y_shap =  0\n",
      "Missing targets in y_shap =  361493\n",
      "Missing targets in shap_values_df =  0\n",
      "Missing targets in shap_values_df =  361493\n",
      "(717148, 265)\n",
      "(717148, 265)\n",
      "[]\n",
      "[]\n",
      "Training until validation scores don't improve for 1 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing targets in y_shap =  0\n",
      "Missing targets in y_shap =  361493\n",
      "Missing targets in shap_values_df =  0\n",
      "Missing targets in shap_values_df =  361493\n",
      "(1434296, 265)\n",
      "(717148, 265)\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_list = feature_selection_shap(X=data,\n",
    "                       features=features,\n",
    "                       target='target',\n",
    "                       params=test_params,\n",
    "                       num_boost_rounds=20,\n",
    "                       sample=0.7,\n",
    "                       num_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resp']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trees_tune(X:pd.DataFrame, target:str, features:list, num_boost_rounds:int, params:dict, learning_rate:float = 0.01, num_iter:int = 10, train_sample:float = 0.7, verbose_eval:int = 0):\n",
    "    \n",
    "    cat_features_indices = [i for i in range(len(features)) if features[i] in cat_features]\n",
    "    params['learning_rate'] = learning_rate\n",
    "    best_iter_list = []\n",
    "        \n",
    "    for i in range(num_iter):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[features], X[target], train_size = train_sample, shuffle = True, stratify = X[target])\n",
    "        if len(cat_features) == 0:\n",
    "            X_train_smote, y_train_smote = SMOTE(random_state=51).fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_train_smote, y_train_smote = SMOTENC(categorical_features=cat_features_indices, random_state=51).fit_resample(X_train, y_train)\n",
    "        \n",
    "        del(X_train, y_train)\n",
    "        gc.collect()\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data = X_train_smote,\n",
    "                                label = y_train_smote,\n",
    "                                feature_name = features,\n",
    "                                categorical_feature = cat_features)\n",
    "        \n",
    "        dval = lgbm.Dataset(data = X_val,\n",
    "                            label = y_val,\n",
    "                            feature_name = features,\n",
    "                            categorical_feature = cat_features)\n",
    "        \n",
    "        del(X_train_smote, y_train_smote, X_val, y_val)\n",
    "        gc.collect()\n",
    "        \n",
    "        current_model = lgbm.train(params=params,\n",
    "                                    train_set=dtrain,\n",
    "                                    num_boost_round=num_boost_rounds,\n",
    "                                    valid_sets=[dval],\n",
    "                                    feature_name=features,\n",
    "                                    categorical_feature=cat_features,\n",
    "                                    early_stopping_rounds=1000,\n",
    "                                    verbose_eval=0)\n",
    "\n",
    "        best_iter_current = current_model.best_iteration\n",
    "        best_iter_list += [best_iter_current]\n",
    "        \n",
    "        print(current_model.best_score)\n",
    "\n",
    "    return int(np.nanmean(best_iter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['target', 'param', 'val'])\n",
    "\n",
    "params = {'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.25,\n",
    "            'is_unbalanced': False}\n",
    "    \n",
    "for target in targets:\n",
    "    train = train_all.copy()\n",
    "    augmented_vals = augmenting_vals(train, target, num = 20)\n",
    "    train = pd.concat([train, augmented_vals], axis = 0, ignore_index = True)\n",
    "    best_params = params_tune(X=train, target=target, features=features, cat_features=cat_features, num_boost_rounds=10000, params=params)\n",
    "    \n",
    "    features_to_keep = feature_selection_shap(X=train[features], y=train[target], params=best_params, features=features, cat_features=cat_features, sample=0.5, num_boost_rounds=10000, num_iter = 3, verbose_eval = 0)\n",
    "    cat_features_to_keep = [x for x in cat_features if x in features_to_keep]\n",
    "    \n",
    "    best_params['features'] = features_to_keep\n",
    "    best_params['cat_features'] = cat_features_to_keep\n",
    "    \n",
    "    num_trees = num_trees_tune(X=train, target=target, features=features_to_keep, cat_features=cat_features_to_keep, num_boost_rounds=50000, params=best_params, learning_rate = 0.01, num_iter = 10, train_sample = 0.7, verbose_eval = 0)\n",
    "    best_params['num_trees'] = int(1.1*num_trees)\n",
    "    \n",
    "    for key in best_params.keys():\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(data=[[target, key, best_params[key]]], columns=['target', 'param', 'val'])], axis=0, ignore_index=True)\n",
    "    print(results_df.tail(10))\n",
    "    print(target, ' completed. ', len(targets) - targets.index(target) - 1, ' to go...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
