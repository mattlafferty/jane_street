{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgbm\n",
    "import optuna as opt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gc\n",
    "import shap\n",
    "from scipy.stats import binom_test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/matthewlafferty/Dropbox/Kaggle/Jane_Street/jane-street-market-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>resp_1</th>\n",
       "      <th>resp_2</th>\n",
       "      <th>resp_3</th>\n",
       "      <th>resp_4</th>\n",
       "      <th>resp</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>ts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>491892</th>\n",
       "      <td>80</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>-0.012751</td>\n",
       "      <td>-0.014573</td>\n",
       "      <td>1</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>-0.788714</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.344326</td>\n",
       "      <td>2.704979</td>\n",
       "      <td>-2.592726</td>\n",
       "      <td>3.000305</td>\n",
       "      <td>-3.388496</td>\n",
       "      <td>3.263304</td>\n",
       "      <td>-2.804504</td>\n",
       "      <td>3.027151</td>\n",
       "      <td>-2.440436</td>\n",
       "      <td>491892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219626</th>\n",
       "      <td>34</td>\n",
       "      <td>0.285587</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.007870</td>\n",
       "      <td>0.015364</td>\n",
       "      <td>0.018541</td>\n",
       "      <td>1</td>\n",
       "      <td>0.491193</td>\n",
       "      <td>0.147054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090701</td>\n",
       "      <td>1.782374</td>\n",
       "      <td>0.395661</td>\n",
       "      <td>1.424740</td>\n",
       "      <td>0.410524</td>\n",
       "      <td>2.460661</td>\n",
       "      <td>0.678450</td>\n",
       "      <td>2.277390</td>\n",
       "      <td>0.635024</td>\n",
       "      <td>219626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015110</th>\n",
       "      <td>429</td>\n",
       "      <td>3.146723</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.001278</td>\n",
       "      <td>-0.002908</td>\n",
       "      <td>-0.004575</td>\n",
       "      <td>-0.002998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.751577</td>\n",
       "      <td>3.397723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744954</td>\n",
       "      <td>0.234358</td>\n",
       "      <td>2.206868</td>\n",
       "      <td>-0.452832</td>\n",
       "      <td>0.602040</td>\n",
       "      <td>-0.132379</td>\n",
       "      <td>1.528540</td>\n",
       "      <td>0.203499</td>\n",
       "      <td>1.997672</td>\n",
       "      <td>2015110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897312</th>\n",
       "      <td>184</td>\n",
       "      <td>0.566679</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.017521</td>\n",
       "      <td>-0.033944</td>\n",
       "      <td>-0.019181</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.416806</td>\n",
       "      <td>-0.403020</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.112464</td>\n",
       "      <td>-1.288962</td>\n",
       "      <td>-2.222993</td>\n",
       "      <td>-0.995549</td>\n",
       "      <td>-3.221498</td>\n",
       "      <td>-1.500098</td>\n",
       "      <td>-2.159030</td>\n",
       "      <td>-1.432691</td>\n",
       "      <td>-2.029860</td>\n",
       "      <td>897312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198769</th>\n",
       "      <td>255</td>\n",
       "      <td>2.800629</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>0.024165</td>\n",
       "      <td>0.045762</td>\n",
       "      <td>0.033423</td>\n",
       "      <td>1</td>\n",
       "      <td>2.093228</td>\n",
       "      <td>0.089239</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360148</td>\n",
       "      <td>1.250625</td>\n",
       "      <td>-0.433197</td>\n",
       "      <td>1.347622</td>\n",
       "      <td>-0.055718</td>\n",
       "      <td>1.910187</td>\n",
       "      <td>-0.133741</td>\n",
       "      <td>1.682871</td>\n",
       "      <td>-0.170468</td>\n",
       "      <td>1198769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455783</th>\n",
       "      <td>72</td>\n",
       "      <td>0.334733</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>-0.014786</td>\n",
       "      <td>-0.019530</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554481</td>\n",
       "      <td>0.870773</td>\n",
       "      <td>-0.790721</td>\n",
       "      <td>0.264029</td>\n",
       "      <td>-2.258830</td>\n",
       "      <td>0.708828</td>\n",
       "      <td>-1.270733</td>\n",
       "      <td>0.877817</td>\n",
       "      <td>-0.874588</td>\n",
       "      <td>455783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337811</th>\n",
       "      <td>284</td>\n",
       "      <td>0.890416</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.043398</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.093788</td>\n",
       "      <td>0.817239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055214</td>\n",
       "      <td>0.476501</td>\n",
       "      <td>0.331179</td>\n",
       "      <td>0.251048</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.229638</td>\n",
       "      <td>-0.167930</td>\n",
       "      <td>0.451875</td>\n",
       "      <td>0.179533</td>\n",
       "      <td>1337811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311783</th>\n",
       "      <td>486</td>\n",
       "      <td>0.077688</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3.172026</td>\n",
       "      <td>-3.093182</td>\n",
       "      <td>...</td>\n",
       "      <td>5.297500</td>\n",
       "      <td>-1.530664</td>\n",
       "      <td>1.852107</td>\n",
       "      <td>-0.688117</td>\n",
       "      <td>5.537706</td>\n",
       "      <td>-1.193737</td>\n",
       "      <td>4.159283</td>\n",
       "      <td>-1.201244</td>\n",
       "      <td>3.337126</td>\n",
       "      <td>2311783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339810</th>\n",
       "      <td>285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002936</td>\n",
       "      <td>-0.002981</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>0.009534</td>\n",
       "      <td>-1</td>\n",
       "      <td>2.014357</td>\n",
       "      <td>3.461213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.874482</td>\n",
       "      <td>-0.048451</td>\n",
       "      <td>1.356119</td>\n",
       "      <td>-0.625356</td>\n",
       "      <td>1.936857</td>\n",
       "      <td>-0.516289</td>\n",
       "      <td>1.817743</td>\n",
       "      <td>-0.378154</td>\n",
       "      <td>1339810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535660</th>\n",
       "      <td>87</td>\n",
       "      <td>1.416155</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.001050</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>-0.005818</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2.276139</td>\n",
       "      <td>-1.557572</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.480704</td>\n",
       "      <td>-2.161251</td>\n",
       "      <td>0.272887</td>\n",
       "      <td>-2.299898</td>\n",
       "      <td>-1.777989</td>\n",
       "      <td>-4.166561</td>\n",
       "      <td>-1.227035</td>\n",
       "      <td>-2.936782</td>\n",
       "      <td>-0.312190</td>\n",
       "      <td>535660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    weight    resp_1    resp_2    resp_3    resp_4      resp  \\\n",
       "491892     80  0.059207  0.003374  0.007049  0.010831 -0.012751 -0.014573   \n",
       "219626     34  0.285587  0.002744  0.004567  0.007870  0.015364  0.018541   \n",
       "2015110   429  3.146723 -0.000063 -0.001278 -0.002908 -0.004575 -0.002998   \n",
       "897312    184  0.566679  0.001359 -0.000410 -0.017521 -0.033944 -0.019181   \n",
       "1198769   255  2.800629  0.001755  0.008786  0.024165  0.045762  0.033423   \n",
       "455783     72  0.334733  0.001044  0.000166  0.002704 -0.014786 -0.019530   \n",
       "1337811   284  0.890416  0.004181  0.009401  0.027605  0.043398  0.030062   \n",
       "2311783   486  0.077688 -0.000734  0.000584  0.004784  0.009934  0.007780   \n",
       "1339810   285  0.000000 -0.002936 -0.002981  0.001015  0.010478  0.009534   \n",
       "535660     87  1.416155 -0.000310 -0.001555 -0.001050 -0.004225 -0.005818   \n",
       "\n",
       "         feature_0  feature_1  feature_2  ...  feature_121  feature_122  \\\n",
       "491892           1   1.107300  -0.788714  ...    -3.344326     2.704979   \n",
       "219626           1   0.491193   0.147054  ...    -0.090701     1.782374   \n",
       "2015110          1   0.751577   3.397723  ...     0.744954     0.234358   \n",
       "897312           1  -0.416806  -0.403020  ...    -3.112464    -1.288962   \n",
       "1198769          1   2.093228   0.089239  ...    -0.360148     1.250625   \n",
       "455783           1  -3.172026  -3.093182  ...    -0.554481     0.870773   \n",
       "1337811         -1   1.093788   0.817239  ...     0.055214     0.476501   \n",
       "2311783         -1  -3.172026  -3.093182  ...     5.297500    -1.530664   \n",
       "1339810         -1   2.014357   3.461213  ...          NaN     1.874482   \n",
       "535660          -1  -2.276139  -1.557572  ...    -1.480704    -2.161251   \n",
       "\n",
       "         feature_123  feature_124  feature_125  feature_126  feature_127  \\\n",
       "491892     -2.592726     3.000305    -3.388496     3.263304    -2.804504   \n",
       "219626      0.395661     1.424740     0.410524     2.460661     0.678450   \n",
       "2015110     2.206868    -0.452832     0.602040    -0.132379     1.528540   \n",
       "897312     -2.222993    -0.995549    -3.221498    -1.500098    -2.159030   \n",
       "1198769    -0.433197     1.347622    -0.055718     1.910187    -0.133741   \n",
       "455783     -0.790721     0.264029    -2.258830     0.708828    -1.270733   \n",
       "1337811     0.331179     0.251048     0.184762     0.229638    -0.167930   \n",
       "2311783     1.852107    -0.688117     5.537706    -1.193737     4.159283   \n",
       "1339810    -0.048451     1.356119    -0.625356     1.936857    -0.516289   \n",
       "535660      0.272887    -2.299898    -1.777989    -4.166561    -1.227035   \n",
       "\n",
       "         feature_128  feature_129    ts_id  \n",
       "491892      3.027151    -2.440436   491892  \n",
       "219626      2.277390     0.635024   219626  \n",
       "2015110     0.203499     1.997672  2015110  \n",
       "897312     -1.432691    -2.029860   897312  \n",
       "1198769     1.682871    -0.170468  1198769  \n",
       "455783      0.877817    -0.874588   455783  \n",
       "1337811     0.451875     0.179533  1337811  \n",
       "2311783    -1.201244     3.337126  2311783  \n",
       "1339810     1.817743    -0.378154  1339810  \n",
       "535660     -2.936782    -0.312190   535660  \n",
       "\n",
       "[10 rows x 138 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = 1 * (data['resp'] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in list(data) if 'feature' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shadow_features(X:pd.DataFrame, features:list) -> pd.DataFrame:  \n",
    "    for feature in features:\n",
    "        temp = X[feature].to_numpy(copy=True)\n",
    "        np.random.shuffle(temp) # shuffle the values of each feature to all the features\n",
    "        X[feature + '_shadow'] = temp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_tune(X:pd.DataFrame, target:str, features:list, num_boost_rounds:int, params:dict, nfolds:int=3, verbose_eval:bool = True, sample:float = 0.5) -> dict:\n",
    "    \n",
    "    X = X.sample(frac=sample)\n",
    "    \n",
    "    dtrain = lgbm.Dataset(data=X[features],\n",
    "                          label=X[target],\n",
    "                          feature_name=features)\n",
    "    \n",
    "    model = opt.integration.lightgbm.LightGBMTunerCV(params = params,\n",
    "                                                     train_set = dtrain,\n",
    "                                                     num_boost_round = num_boost_rounds,\n",
    "                                                     nfold = nfolds,\n",
    "                                                     stratified = True,\n",
    "                                                     shuffle = True,\n",
    "                                                     feature_name=features,\n",
    "                                                     early_stopping_rounds=0.05*num_boost_rounds,\n",
    "                                                     verbose_eval = verbose_eval,\n",
    "                                                     seed = 51)\n",
    "    model.run()\n",
    "    print(model.best_score)\n",
    "    return model.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2021-01-10 08:19:50,531] A new study created in memory with name: no-name-9120fd8b-44a7-461c-aa31-6850cc0b32c8\n",
      "\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: -inf:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: 0.537516:   0%|          | 0/7 [00:41<?, ?it/s]\n",
      "\n",
      "feature_fraction, val_score: 0.537516:  14%|#4        | 1/7 [00:41<04:07, 41.30s/it][I 2021-01-10 08:20:31,838] Trial 0 finished with value: 0.5375160768507232 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.5375160768507232.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.537516:  14%|#4        | 1/7 [00:41<04:07, 41.30s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  14%|#4        | 1/7 [01:45<04:07, 41.30s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  29%|##8       | 2/7 [01:45<04:01, 48.32s/it][I 2021-01-10 08:21:36,527] Trial 1 finished with value: 0.5388134106108794 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.5388134106108794.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.538813:  29%|##8       | 2/7 [01:45<04:01, 48.32s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  29%|##8       | 2/7 [04:22<04:01, 48.32s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [04:22<05:23, 80.79s/it][I 2021-01-10 08:24:13,085] Trial 2 finished with value: 0.5429810769415407 and parameters: {'feature_fraction': 0.6}. Best is trial 2 with value: 0.5429810769415407.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [04:22<05:23, 80.79s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  43%|####2     | 3/7 [05:39<05:23, 80.79s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  57%|#####7    | 4/7 [05:39<03:58, 79.50s/it][I 2021-01-10 08:25:29,594] Trial 3 finished with value: 0.5417169069269036 and parameters: {'feature_fraction': 0.4}. Best is trial 2 with value: 0.5429810769415407.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.542981:  57%|#####7    | 4/7 [05:39<03:58, 79.50s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  57%|#####7    | 4/7 [11:20<03:58, 79.50s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [11:20<05:16, 158.19s/it][I 2021-01-10 08:31:11,399] Trial 4 finished with value: 0.5464531535040185 and parameters: {'feature_fraction': 0.7}. Best is trial 4 with value: 0.5464531535040185.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [11:20<05:16, 158.19s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  71%|#######1  | 5/7 [12:05<05:16, 158.19s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:05<02:04, 124.09s/it][I 2021-01-10 08:31:55,916] Trial 5 finished with value: 0.5398975385133017 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 4 with value: 0.5464531535040185.\n",
      "\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:05<02:04, 124.09s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453:  86%|########5 | 6/7 [12:36<02:04, 124.09s/it]\n",
      "\n",
      "feature_fraction, val_score: 0.546453: 100%|##########| 7/7 [12:36<00:00, 96.10s/it] [I 2021-01-10 08:32:26,687] Trial 6 finished with value: 0.5377752471851088 and parameters: {'feature_fraction': 0.5}. Best is trial 4 with value: 0.5464531535040185.\n",
      "feature_fraction, val_score: 0.546453: 100%|##########| 7/7 [12:36<00:00, 108.02s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.546453:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   0%|          | 0/20 [12:04<?, ?it/s]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [12:04<3:49:19, 724.20s/it][I 2021-01-10 08:44:30,896] Trial 7 finished with value: 0.5589578208690908 and parameters: {'num_leaves': 143}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [12:04<3:49:19, 724.20s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:   5%|5         | 1/20 [19:25<3:49:19, 724.20s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  10%|#         | 2/20 [19:25<3:11:48, 639.39s/it][I 2021-01-10 08:51:52,391] Trial 8 finished with value: 0.5512564187373068 and parameters: {'num_leaves': 64}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  10%|#         | 2/20 [19:25<3:11:48, 639.39s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  10%|#         | 2/20 [28:45<3:11:48, 639.39s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  15%|#5        | 3/20 [28:45<2:54:21, 615.38s/it][I 2021-01-10 09:01:11,739] Trial 9 finished with value: 0.5555930582471985 and parameters: {'num_leaves': 93}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  15%|#5        | 3/20 [28:45<2:54:21, 615.38s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  15%|#5        | 3/20 [43:29<2:54:21, 615.38s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  20%|##        | 4/20 [43:29<3:05:38, 696.18s/it][I 2021-01-10 09:15:56,479] Trial 10 finished with value: 0.5585754248848201 and parameters: {'num_leaves': 256}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  20%|##        | 4/20 [43:29<3:05:38, 696.18s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  20%|##        | 4/20 [57:22<3:05:38, 696.18s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  25%|##5       | 5/20 [57:22<3:04:18, 737.25s/it][I 2021-01-10 09:29:49,535] Trial 11 finished with value: 0.5585754248848201 and parameters: {'num_leaves': 256}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  25%|##5       | 5/20 [57:22<3:04:18, 737.25s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  25%|##5       | 5/20 [1:14:42<3:04:18, 737.25s/it]\n",
      "\n",
      "num_leaves, val_score: 0.558958:  30%|###       | 6/20 [1:14:42<3:13:13, 828.07s/it][I 2021-01-10 09:47:09,534] Trial 12 finished with value: 0.5587865549369643 and parameters: {'num_leaves': 236}. Best is trial 7 with value: 0.5589578208690908.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.558958:  30%|###       | 6/20 [1:14:42<3:13:13, 828.07s/it]\n",
      "\n",
      "num_leaves, val_score: 0.559418:  30%|###       | 6/20 [1:31:11<3:13:13, 828.07s/it]\n",
      "\n",
      "num_leaves, val_score: 0.559418:  35%|###5      | 7/20 [1:31:11<3:09:51, 876.27s/it][I 2021-01-10 10:03:38,265] Trial 13 finished with value: 0.5594176962340017 and parameters: {'num_leaves': 182}. Best is trial 13 with value: 0.5594176962340017.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.559418:  35%|###5      | 7/20 [1:31:11<3:09:51, 876.27s/it]\n",
      "\n",
      "num_leaves, val_score: 0.559637:  35%|###5      | 7/20 [1:48:36<3:09:51, 876.27s/it]\n",
      "\n",
      "num_leaves, val_score: 0.559637:  40%|####      | 8/20 [1:48:36<3:05:22, 926.90s/it][I 2021-01-10 10:21:03,302] Trial 14 finished with value: 0.5596372691696466 and parameters: {'num_leaves': 175}. Best is trial 14 with value: 0.5596372691696466.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.559637:  40%|####      | 8/20 [1:48:36<3:05:22, 926.90s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  40%|####      | 8/20 [2:04:42<3:05:22, 926.90s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  45%|####5     | 9/20 [2:04:42<2:52:03, 938.48s/it][I 2021-01-10 10:37:08,811] Trial 15 finished with value: 0.5606214028453874 and parameters: {'num_leaves': 189}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  45%|####5     | 9/20 [2:04:42<2:52:03, 938.48s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  45%|####5     | 9/20 [2:20:39<2:52:03, 938.48s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  50%|#####     | 10/20 [2:20:39<2:37:22, 944.28s/it][I 2021-01-10 10:53:06,613] Trial 16 finished with value: 0.5594867948961001 and parameters: {'num_leaves': 196}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  50%|#####     | 10/20 [2:20:39<2:37:22, 944.28s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  50%|#####     | 10/20 [2:21:22<2:37:22, 944.28s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  55%|#####5    | 11/20 [2:21:22<1:41:03, 673.70s/it][I 2021-01-10 10:53:48,965] Trial 17 finished with value: 0.5372052142527722 and parameters: {'num_leaves': 3}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  55%|#####5    | 11/20 [2:21:22<1:41:03, 673.70s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  55%|#####5    | 11/20 [2:34:34<1:41:03, 673.70s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  60%|######    | 12/20 [2:34:34<1:34:34, 709.35s/it][I 2021-01-10 11:07:01,505] Trial 18 finished with value: 0.5594217323359673 and parameters: {'num_leaves': 183}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  60%|######    | 12/20 [2:34:34<1:34:34, 709.35s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  60%|######    | 12/20 [2:46:12<1:34:34, 709.35s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  65%|######5   | 13/20 [2:46:12<1:22:20, 705.77s/it][I 2021-01-10 11:18:38,929] Trial 19 finished with value: 0.5594370688093795 and parameters: {'num_leaves': 137}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  65%|######5   | 13/20 [2:46:12<1:22:20, 705.77s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  65%|######5   | 13/20 [3:04:39<1:22:20, 705.77s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  70%|#######   | 14/20 [3:04:39<1:22:37, 826.18s/it][I 2021-01-10 11:37:06,049] Trial 20 finished with value: 0.5583502539526152 and parameters: {'num_leaves': 214}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  70%|#######   | 14/20 [3:04:39<1:22:37, 826.18s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  70%|#######   | 14/20 [3:20:18<1:22:37, 826.18s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  75%|#######5  | 15/20 [3:20:18<1:11:39, 859.95s/it][I 2021-01-10 11:52:44,810] Trial 21 finished with value: 0.5589388007018251 and parameters: {'num_leaves': 186}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  75%|#######5  | 15/20 [3:20:18<1:11:39, 859.95s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  75%|#######5  | 15/20 [3:33:01<1:11:39, 859.95s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  80%|########  | 16/20 [3:33:01<55:24, 831.01s/it]  [I 2021-01-10 12:05:28,289] Trial 22 finished with value: 0.5579686041578692 and parameters: {'num_leaves': 205}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  80%|########  | 16/20 [3:33:01<55:24, 831.01s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  80%|########  | 16/20 [3:46:59<55:24, 831.01s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  85%|########5 | 17/20 [3:46:59<41:39, 833.15s/it][I 2021-01-10 12:19:26,416] Trial 23 finished with value: 0.5586875559796769 and parameters: {'num_leaves': 153}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  85%|########5 | 17/20 [3:46:59<41:39, 833.15s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  85%|########5 | 17/20 [4:04:27<41:39, 833.15s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  90%|######### | 18/20 [4:04:27<29:54, 897.49s/it][I 2021-01-10 12:36:54,050] Trial 24 finished with value: 0.5577617250548292 and parameters: {'num_leaves': 164}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  90%|######### | 18/20 [4:04:27<29:54, 897.49s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  90%|######### | 18/20 [4:15:28<29:54, 897.49s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  95%|#########5| 19/20 [4:15:28<13:46, 826.68s/it][I 2021-01-10 12:47:55,487] Trial 25 finished with value: 0.5543594349593172 and parameters: {'num_leaves': 106}. Best is trial 15 with value: 0.5606214028453874.\n",
      "\n",
      "\n",
      "num_leaves, val_score: 0.560621:  95%|#########5| 19/20 [4:15:28<13:46, 826.68s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621:  95%|#########5| 19/20 [4:31:03<13:46, 826.68s/it]\n",
      "\n",
      "num_leaves, val_score: 0.560621: 100%|##########| 20/20 [4:31:03<00:00, 859.22s/it][I 2021-01-10 13:03:30,648] Trial 26 finished with value: 0.5589511982007912 and parameters: {'num_leaves': 220}. Best is trial 15 with value: 0.5606214028453874.\n",
      "num_leaves, val_score: 0.560621: 100%|##########| 20/20 [4:31:03<00:00, 813.20s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "bagging, val_score: 0.560621:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\n",
      "bagging, val_score: 0.560621:   0%|          | 0/10 [07:05<?, ?it/s]\n",
      "\n",
      "bagging, val_score: 0.560621:  10%|#         | 1/10 [07:05<1:03:51, 425.70s/it][I 2021-01-10 13:10:36,385] Trial 27 finished with value: 0.54314315716091 and parameters: {'bagging_fraction': 0.6759759498436937, 'bagging_freq': 5}. Best is trial 27 with value: 0.54314315716091.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  10%|#         | 1/10 [07:05<1:03:51, 425.70s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  10%|#         | 1/10 [25:24<1:03:51, 425.70s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  20%|##        | 2/10 [25:24<1:23:42, 627.77s/it][I 2021-01-10 13:28:55,653] Trial 28 finished with value: 0.5575408787876089 and parameters: {'bagging_fraction': 0.9764725982954103, 'bagging_freq': 1}. Best is trial 28 with value: 0.5575408787876089.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  20%|##        | 2/10 [25:24<1:23:42, 627.77s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  20%|##        | 2/10 [33:03<1:23:42, 627.77s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  30%|###       | 3/10 [33:03<1:07:18, 576.96s/it][I 2021-01-10 13:36:34,050] Trial 29 finished with value: 0.5369689101224563 and parameters: {'bagging_fraction': 0.4315005027839142, 'bagging_freq': 7}. Best is trial 28 with value: 0.5575408787876089.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  30%|###       | 3/10 [33:03<1:07:18, 576.96s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  30%|###       | 3/10 [51:24<1:07:18, 576.96s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  40%|####      | 4/10 [51:24<1:13:26, 734.34s/it][I 2021-01-10 13:54:55,622] Trial 30 finished with value: 0.5587604227623636 and parameters: {'bagging_fraction': 0.9929391596324885, 'bagging_freq': 1}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  40%|####      | 4/10 [51:24<1:13:26, 734.34s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  40%|####      | 4/10 [59:15<1:13:26, 734.34s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  50%|#####     | 5/10 [59:15<54:35, 655.07s/it]  [I 2021-01-10 14:02:45,705] Trial 31 finished with value: 0.537517061199246 and parameters: {'bagging_fraction': 0.4383261259076247, 'bagging_freq': 4}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  50%|#####     | 5/10 [59:15<54:35, 655.07s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  50%|#####     | 5/10 [1:06:36<54:35, 655.07s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  60%|######    | 6/10 [1:06:36<39:23, 590.86s/it][I 2021-01-10 14:10:06,757] Trial 32 finished with value: 0.5456991120830637 and parameters: {'bagging_fraction': 0.7420105719587419, 'bagging_freq': 7}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  60%|######    | 6/10 [1:06:36<39:23, 590.86s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  60%|######    | 6/10 [1:17:26<39:23, 590.86s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  70%|#######   | 7/10 [1:17:26<30:26, 608.81s/it][I 2021-01-10 14:20:57,437] Trial 33 finished with value: 0.5481552748146524 and parameters: {'bagging_fraction': 0.6607061256798454, 'bagging_freq': 2}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  70%|#######   | 7/10 [1:17:26<30:26, 608.81s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  70%|#######   | 7/10 [1:34:45<30:26, 608.81s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  80%|########  | 8/10 [1:34:45<24:35, 737.68s/it][I 2021-01-10 14:38:15,821] Trial 34 finished with value: 0.5545744454910125 and parameters: {'bagging_fraction': 0.8346093078996395, 'bagging_freq': 3}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  80%|########  | 8/10 [1:34:45<24:35, 737.68s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  80%|########  | 8/10 [1:42:28<24:35, 737.68s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  90%|######### | 9/10 [1:42:28<10:55, 655.48s/it][I 2021-01-10 14:45:59,488] Trial 35 finished with value: 0.5398211888127777 and parameters: {'bagging_fraction': 0.5330867619325294, 'bagging_freq': 5}. Best is trial 30 with value: 0.5587604227623636.\n",
      "\n",
      "\n",
      "bagging, val_score: 0.560621:  90%|######### | 9/10 [1:42:28<10:55, 655.48s/it]\n",
      "\n",
      "bagging, val_score: 0.560621:  90%|######### | 9/10 [1:58:51<10:55, 655.48s/it]\n",
      "\n",
      "bagging, val_score: 0.560621: 100%|##########| 10/10 [1:58:51<00:00, 753.71s/it][I 2021-01-10 15:02:22,402] Trial 36 finished with value: 0.5569768185217205 and parameters: {'bagging_fraction': 0.8728448138830032, 'bagging_freq': 6}. Best is trial 30 with value: 0.5587604227623636.\n",
      "bagging, val_score: 0.560621: 100%|##########| 10/10 [1:58:51<00:00, 713.17s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:   0%|          | 0/6 [15:59<?, ?it/s]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  17%|#6        | 1/6 [15:59<1:19:55, 959.00s/it][I 2021-01-10 15:18:21,422] Trial 37 finished with value: 0.5587881840369199 and parameters: {'feature_fraction': 0.6839999999999999}. Best is trial 37 with value: 0.5587881840369199.\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  17%|#6        | 1/6 [15:59<1:19:55, 959.00s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  17%|#6        | 1/6 [32:18<1:19:55, 959.00s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  33%|###3      | 2/6 [32:18<1:04:20, 965.04s/it][I 2021-01-10 15:34:40,552] Trial 38 finished with value: 0.5602821331713637 and parameters: {'feature_fraction': 0.716}. Best is trial 38 with value: 0.5602821331713637.\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  33%|###3      | 2/6 [32:18<1:04:20, 965.04s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  33%|###3      | 2/6 [47:39<1:04:20, 965.04s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  50%|#####     | 3/6 [47:39<47:35, 951.95s/it]  [I 2021-01-10 15:50:01,942] Trial 39 finished with value: 0.5599348948316683 and parameters: {'feature_fraction': 0.652}. Best is trial 38 with value: 0.5602821331713637.\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  50%|#####     | 3/6 [47:39<47:35, 951.95s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  50%|#####     | 3/6 [1:04:42<47:35, 951.95s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  67%|######6   | 4/6 [1:04:42<32:26, 973.23s/it][I 2021-01-10 16:07:04,848] Trial 40 finished with value: 0.5591417697129514 and parameters: {'feature_fraction': 0.748}. Best is trial 38 with value: 0.5602821331713637.\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  67%|######6   | 4/6 [1:04:42<32:26, 973.23s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  67%|######6   | 4/6 [1:18:45<32:26, 973.23s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  83%|########3 | 5/6 [1:18:45<15:34, 934.33s/it][I 2021-01-10 16:21:08,386] Trial 41 finished with value: 0.5586550728984553 and parameters: {'feature_fraction': 0.62}. Best is trial 38 with value: 0.5602821331713637.\n",
      "\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  83%|########3 | 5/6 [1:18:45<15:34, 934.33s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621:  83%|########3 | 5/6 [1:35:48<15:34, 934.33s/it]\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.560621: 100%|##########| 6/6 [1:35:48<00:00, 960.88s/it][I 2021-01-10 16:38:11,221] Trial 42 finished with value: 0.5597201644623219 and parameters: {'feature_fraction': 0.7799999999999999}. Best is trial 38 with value: 0.5602821331713637.\n",
      "feature_fraction_stage2, val_score: 0.560621: 100%|##########| 6/6 [1:35:48<00:00, 958.13s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:   0%|          | 0/20 [06:05<?, ?it/s]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:   5%|5         | 1/20 [06:05<1:55:36, 365.10s/it][I 2021-01-10 16:44:16,327] Trial 43 finished with value: 0.5466544927851388 and parameters: {'lambda_l1': 9.28226098730975, 'lambda_l2': 2.455683506220272e-08}. Best is trial 43 with value: 0.5466544927851388.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:   5%|5         | 1/20 [06:05<1:55:36, 365.10s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:   5%|5         | 1/20 [21:27<1:55:36, 365.10s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  10%|#         | 2/20 [21:27<2:39:42, 532.35s/it][I 2021-01-10 16:59:38,925] Trial 44 finished with value: 0.5542439801393222 and parameters: {'lambda_l1': 1.3520092805544684e-08, 'lambda_l2': 7.781459309403786}. Best is trial 44 with value: 0.5542439801393222.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  10%|#         | 2/20 [21:27<2:39:42, 532.35s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  10%|#         | 2/20 [38:02<2:39:42, 532.35s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  15%|#5        | 3/20 [38:02<3:10:08, 671.08s/it][I 2021-01-10 17:16:13,732] Trial 45 finished with value: 0.559377278089789 and parameters: {'lambda_l1': 0.0008562537633477875, 'lambda_l2': 0.0029741574885721184}. Best is trial 45 with value: 0.559377278089789.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  15%|#5        | 3/20 [38:02<3:10:08, 671.08s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  15%|#5        | 3/20 [51:46<3:10:08, 671.08s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  20%|##        | 4/20 [51:46<3:11:10, 716.92s/it][I 2021-01-10 17:29:57,586] Trial 46 finished with value: 0.5599632017297497 and parameters: {'lambda_l1': 4.058628868815585e-08, 'lambda_l2': 8.161957343418397e-08}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  20%|##        | 4/20 [51:46<3:11:10, 716.92s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  20%|##        | 4/20 [1:07:49<3:11:10, 716.92s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  25%|##5       | 5/20 [1:07:49<3:17:43, 790.89s/it][I 2021-01-10 17:46:01,078] Trial 47 finished with value: 0.5596857471268021 and parameters: {'lambda_l1': 1.9396940173320218e-08, 'lambda_l2': 1.4557662717295937e-08}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  25%|##5       | 5/20 [1:07:49<3:17:43, 790.89s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  25%|##5       | 5/20 [1:16:41<3:17:43, 790.89s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  30%|###       | 6/20 [1:16:41<2:46:25, 713.25s/it][I 2021-01-10 17:54:53,157] Trial 48 finished with value: 0.5557146211944475 and parameters: {'lambda_l1': 2.0163079546954744e-05, 'lambda_l2': 1.6434584741517385e-05}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  30%|###       | 6/20 [1:16:41<2:46:25, 713.25s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  30%|###       | 6/20 [1:32:05<2:46:25, 713.25s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  35%|###5      | 7/20 [1:32:05<2:48:13, 776.40s/it][I 2021-01-10 18:10:16,908] Trial 49 finished with value: 0.5598858723010713 and parameters: {'lambda_l1': 8.680751716915396e-06, 'lambda_l2': 7.4141545460408865e-06}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  35%|###5      | 7/20 [1:32:05<2:48:13, 776.40s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  35%|###5      | 7/20 [1:48:21<2:48:13, 776.40s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  40%|####      | 8/20 [1:48:21<2:47:14, 836.18s/it][I 2021-01-10 18:26:32,578] Trial 50 finished with value: 0.5597804648925727 and parameters: {'lambda_l1': 6.972198446083044e-06, 'lambda_l2': 7.025664284171402e-06}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  40%|####      | 8/20 [1:48:21<2:47:14, 836.18s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  40%|####      | 8/20 [2:04:35<2:47:14, 836.18s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  45%|####5     | 9/20 [2:04:35<2:40:54, 877.66s/it][I 2021-01-10 18:42:47,017] Trial 51 finished with value: 0.5593662715317803 and parameters: {'lambda_l1': 9.417527833272214e-06, 'lambda_l2': 9.578447167258544e-06}. Best is trial 46 with value: 0.5599632017297497.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  45%|####5     | 9/20 [2:04:35<2:40:54, 877.66s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  45%|####5     | 9/20 [2:18:21<2:40:54, 877.66s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  50%|#####     | 10/20 [2:18:21<2:23:40, 862.04s/it][I 2021-01-10 18:56:32,609] Trial 52 finished with value: 0.5605982220362212 and parameters: {'lambda_l1': 8.966114250526061e-07, 'lambda_l2': 6.903356896683898e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  50%|#####     | 10/20 [2:18:21<2:23:40, 862.04s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  50%|#####     | 10/20 [2:34:21<2:23:40, 862.04s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  55%|#####5    | 11/20 [2:34:21<2:13:43, 891.47s/it][I 2021-01-10 19:12:32,756] Trial 53 finished with value: 0.5597727385862342 and parameters: {'lambda_l1': 1.9000344529246053e-07, 'lambda_l2': 8.902740459431673e-08}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  55%|#####5    | 11/20 [2:34:21<2:13:43, 891.47s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  55%|#####5    | 11/20 [2:50:37<2:13:43, 891.47s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  60%|######    | 12/20 [2:50:37<2:02:15, 916.97s/it][I 2021-01-10 19:28:49,216] Trial 54 finished with value: 0.5604639965488635 and parameters: {'lambda_l1': 2.8562524693871136e-07, 'lambda_l2': 4.533566939418344e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  60%|######    | 12/20 [2:50:37<2:02:15, 916.97s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  60%|######    | 12/20 [3:06:29<2:02:15, 916.97s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  65%|######5   | 13/20 [3:06:29<1:48:11, 927.37s/it][I 2021-01-10 19:44:40,861] Trial 55 finished with value: 0.5604225528928443 and parameters: {'lambda_l1': 2.0625826558499224e-07, 'lambda_l2': 2.7809035142660023e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  65%|######5   | 13/20 [3:06:29<1:48:11, 927.37s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  65%|######5   | 13/20 [3:22:01<1:48:11, 927.37s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  70%|#######   | 14/20 [3:22:01<1:32:52, 928.72s/it][I 2021-01-10 20:00:12,739] Trial 56 finished with value: 0.5594678400440446 and parameters: {'lambda_l1': 2.1170009825972834e-07, 'lambda_l2': 2.6128961393124296e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  70%|#######   | 14/20 [3:22:01<1:32:52, 928.72s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  70%|#######   | 14/20 [3:38:18<1:32:52, 928.72s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  75%|#######5  | 15/20 [3:38:18<1:18:36, 943.28s/it][I 2021-01-10 20:16:29,992] Trial 57 finished with value: 0.5599831413503616 and parameters: {'lambda_l1': 2.673718506926586e-07, 'lambda_l2': 4.2906125683318926e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  75%|#######5  | 15/20 [3:38:18<1:18:36, 943.28s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  75%|#######5  | 15/20 [3:50:27<1:18:36, 943.28s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  80%|########  | 16/20 [3:50:27<58:35, 878.85s/it]  [I 2021-01-10 20:28:38,517] Trial 58 finished with value: 0.5602738874935943 and parameters: {'lambda_l1': 5.138184500777045e-07, 'lambda_l2': 6.9612519714112e-07}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  80%|########  | 16/20 [3:50:27<58:35, 878.85s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  80%|########  | 16/20 [4:05:55<58:35, 878.85s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  85%|########5 | 17/20 [4:05:55<44:41, 893.76s/it][I 2021-01-10 20:44:07,066] Trial 59 finished with value: 0.5593953237793192 and parameters: {'lambda_l1': 1.2244136686679625e-06, 'lambda_l2': 1.1737084734718952e-06}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  85%|########5 | 17/20 [4:05:55<44:41, 893.76s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  85%|########5 | 17/20 [4:21:51<44:41, 893.76s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  90%|######### | 18/20 [4:21:51<30:24, 912.42s/it][I 2021-01-10 21:00:03,009] Trial 60 finished with value: 0.5577810978402239 and parameters: {'lambda_l1': 0.0005867870574620406, 'lambda_l2': 0.00012151763079202425}. Best is trial 52 with value: 0.5605982220362212.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560621:  90%|######### | 18/20 [4:21:51<30:24, 912.42s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560824:  90%|######### | 18/20 [4:37:29<30:24, 912.42s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560824:  95%|#########5| 19/20 [4:37:29<15:19, 919.97s/it][I 2021-01-10 21:15:40,606] Trial 61 finished with value: 0.5608243819373179 and parameters: {'lambda_l1': 4.805704302063685e-07, 'lambda_l2': 6.497878684050329e-07}. Best is trial 61 with value: 0.5608243819373179.\n",
      "\n",
      "\n",
      "regularization_factors, val_score: 0.560824:  95%|#########5| 19/20 [4:37:29<15:19, 919.97s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560824:  95%|#########5| 19/20 [4:53:46<15:19, 919.97s/it]\n",
      "\n",
      "regularization_factors, val_score: 0.560824: 100%|##########| 20/20 [4:53:46<00:00, 937.07s/it][I 2021-01-10 21:31:57,582] Trial 62 finished with value: 0.5606487869298546 and parameters: {'lambda_l1': 1.1485041880082057e-06, 'lambda_l2': 8.426640630148311e-07}. Best is trial 61 with value: 0.5608243819373179.\n",
      "regularization_factors, val_score: 0.560824: 100%|##########| 20/20 [4:53:46<00:00, 881.32s/it]\n",
      "\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:   0%|          | 0/5 [00:56<?, ?it/s]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  20%|##        | 1/5 [00:56<03:47, 56.84s/it][I 2021-01-10 21:32:54,436] Trial 63 finished with value: 0.5374291468211913 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.5374291468211913.\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  20%|##        | 1/5 [00:56<03:47, 56.84s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  20%|##        | 1/5 [14:59<03:47, 56.84s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  40%|####      | 2/5 [14:59<14:37, 292.52s/it][I 2021-01-10 21:46:56,855] Trial 64 finished with value: 0.5577034905593808 and parameters: {'min_child_samples': 25}. Best is trial 64 with value: 0.5577034905593808.\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  40%|####      | 2/5 [14:59<14:37, 292.52s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  40%|####      | 2/5 [30:29<14:37, 292.52s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  60%|######    | 3/5 [30:29<16:07, 483.80s/it][I 2021-01-10 22:02:26,988] Trial 65 finished with value: 0.5577380576368457 and parameters: {'min_child_samples': 50}. Best is trial 65 with value: 0.5577380576368457.\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  60%|######    | 3/5 [30:29<16:07, 483.80s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  60%|######    | 3/5 [46:15<16:07, 483.80s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  80%|########  | 4/5 [46:15<10:22, 622.35s/it][I 2021-01-10 22:18:12,609] Trial 66 finished with value: 0.5572491737078805 and parameters: {'min_child_samples': 100}. Best is trial 65 with value: 0.5577380576368457.\n",
      "\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  80%|########  | 4/5 [46:15<10:22, 622.35s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824:  80%|########  | 4/5 [1:03:26<10:22, 622.35s/it]\n",
      "\n",
      "min_data_in_leaf, val_score: 0.560824: 100%|##########| 5/5 [1:03:26<00:00, 745.21s/it][I 2021-01-10 22:35:24,501] Trial 67 finished with value: 0.5587631813324186 and parameters: {'min_child_samples': 10}. Best is trial 67 with value: 0.5587631813324186.\n",
      "min_data_in_leaf, val_score: 0.560824: 100%|##########| 5/5 [1:03:26<00:00, 761.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5608243819373179\n"
     ]
    }
   ],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'verbosity': -1,\n",
    "          'boosting_type': 'gbdt',\n",
    "          'learning_rate': 0.25,\n",
    "          'is_unbalanced': False,\n",
    "          'metric': 'auc'}\n",
    "\n",
    "params_0 = params_tune(X=data,\n",
    "                       target='target',\n",
    "                       features=[x for x in features if x not in ['resp','weight']],\n",
    "                       num_boost_rounds=5000,\n",
    "                       params=params,\n",
    "                       nfolds=3,\n",
    "                       verbose_eval=False,\n",
    "                       sample = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'verbosity': -1,\n",
       " 'boosting_type': 'gbdt',\n",
       " 'learning_rate': 0.25,\n",
       " 'is_unbalanced': False,\n",
       " 'metric': 'auc',\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 4.805704302063685e-07,\n",
       " 'lambda_l2': 6.497878684050329e-07,\n",
       " 'num_leaves': 189,\n",
       " 'feature_fraction': 0.7,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 20}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_0['metric'] = 'auc'\n",
    "params_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_shap(X:pd.DataFrame, features:list, target:str, params:dict, num_boost_rounds:int=10000, sample:float = 0.7, num_iter:int = 3) -> list:\n",
    "    \n",
    "    # We will determine optimal parameters on a random sample. We'll use these parameters to construct a new model for each iteration.\n",
    "    # Determining the hyperparameters takes time, so we'll do it once, and use these parameters to construct our models.\n",
    "    X_shadow = create_shadow_features(X, features)\n",
    "    \n",
    "    # These are the features, categorical features, and categorical feature indices that we will use once the shadow features are added.\n",
    "    shadow_features = features + [x + '_shadow' for x in features]\n",
    "                        \n",
    "    # This is the data frame that we will use to store our SHAP value information\n",
    "    results_df = pd.DataFrame(columns = shadow_features + [target])\n",
    "    \n",
    "    # With the parameters determined, we'll build models using the parameters on a random sample of records, determine the SHAP values on\n",
    "    # the remaining records.\n",
    "    for i in range(num_iter):\n",
    "        X_train, X_shap, y_train, y_shap = train_test_split(X_shadow[shadow_features], X_shadow[target], train_size=sample, shuffle=True, stratify=X_shadow[target])\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data = X_train,\n",
    "                              label = y_train,\n",
    "                              feature_name = shadow_features)\n",
    "        \n",
    "        dval = lgbm.Dataset(data = X_shap,\n",
    "                            label = y_shap,\n",
    "                            feature_name = shadow_features)\n",
    "        \n",
    "        del(X_train, y_train)\n",
    "        gc.collect()\n",
    "        \n",
    "        current_model = lgbm.train(params=params,\n",
    "                                   train_set=dtrain,\n",
    "                                   num_boost_round=num_boost_rounds,\n",
    "                                   valid_sets=dval,\n",
    "                                   feature_name=shadow_features,\n",
    "                                   early_stopping_rounds=int(0.05*num_boost_rounds),\n",
    "                                   verbose_eval=100)\n",
    "        \n",
    "        shap_values = shap.TreeExplainer(current_model).shap_values(X_shap)\n",
    "        shap_values_df = pd.DataFrame(data = shap_values[1], columns = shadow_features)\n",
    "        shap_values_df[target] = np.where(y_shap == 1, 1, 0)\n",
    "        \n",
    "        print('Missing targets in y_shap = ', sum(y_shap.isna()))\n",
    "        print('Missing targets in y_shap = ', sum(y_shap))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target'].isna()))\n",
    "        print('Missing targets in shap_values_df = ', sum(shap_values_df['target']))\n",
    "        \n",
    "        #print(results.shape)\n",
    "        #print(np.abs(shap_values[0]).mean(0))\n",
    "        print(results_df.shape)\n",
    "        #print(results_df.sample(10))\n",
    "        print(shap_values_df.shape)\n",
    "        print([x for x in list(results_df) if x not in list(shap_values_df)])\n",
    "        print([x for x in list(shap_values_df) if x not in list(results_df)])\n",
    "        #print(shap_values_df.sample(10))\n",
    "        results_df = pd.concat([results_df, shap_values_df], axis = 0)\n",
    "    \n",
    "    cols_to_keep = []\n",
    "    n = results_df.shape[0]\n",
    "    for col in features:\n",
    "        x = np.sum(np.where((results_df[target] == 0) & (results_df[col] < results_df[col + '_shadow']), 1,\n",
    "                        np.where((results_df[target] == 1) & (results_df[col] > results_df[col + '_shadow']), 1, 0)))\n",
    "        p_val = binom_test(x=x, n=n, p=0.5, alternative='greater')\n",
    "        if p_val <= 0.05:\n",
    "            cols_to_keep += [col]\n",
    "            \n",
    "    return cols_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\tvalid_0's auc: 0.615545\n",
      "[200]\tvalid_0's auc: 0.620539\n",
      "[300]\tvalid_0's auc: 0.622757\n",
      "[400]\tvalid_0's auc: 0.621391\n",
      "[500]\tvalid_0's auc: 0.62185\n",
      "[600]\tvalid_0's auc: 0.622995\n",
      "[700]\tvalid_0's auc: 0.624099\n",
      "[800]\tvalid_0's auc: 0.622831\n",
      "[900]\tvalid_0's auc: 0.625327\n",
      "[1000]\tvalid_0's auc: 0.624036\n",
      "[1100]\tvalid_0's auc: 0.625355\n",
      "[1200]\tvalid_0's auc: 0.626502\n",
      "[1300]\tvalid_0's auc: 0.626611\n",
      "[1400]\tvalid_0's auc: 0.626031\n",
      "[1500]\tvalid_0's auc: 0.626124\n",
      "[1600]\tvalid_0's auc: 0.625573\n",
      "[1700]\tvalid_0's auc: 0.625923\n",
      "[1800]\tvalid_0's auc: 0.625734\n",
      "[1900]\tvalid_0's auc: 0.626323\n",
      "[2000]\tvalid_0's auc: 0.6267\n",
      "[2100]\tvalid_0's auc: 0.627149\n",
      "[2200]\tvalid_0's auc: 0.626988\n",
      "[2300]\tvalid_0's auc: 0.626752\n",
      "[2400]\tvalid_0's auc: 0.626532\n",
      "[2500]\tvalid_0's auc: 0.625905\n",
      "[2600]\tvalid_0's auc: 0.626303\n",
      "[2700]\tvalid_0's auc: 0.626939\n",
      "[2800]\tvalid_0's auc: 0.62694\n",
      "[2900]\tvalid_0's auc: 0.626564\n",
      "[3000]\tvalid_0's auc: 0.627321\n",
      "Early stopping, best iteration is:\n",
      "[2054]\tvalid_0's auc: 0.627538\n"
     ]
    }
   ],
   "source": [
    "features_to_keep = feature_selection_shap(X=data,\n",
    "                                          features=features,\n",
    "                                          target='target',\n",
    "                                          params=params_0,\n",
    "                                          num_boost_rounds=20000,\n",
    "                                          sample=0.7,\n",
    "                                          num_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resp']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgbm.Dataset(data=data[[x for x in features if x not in ['resp','weight']]],\n",
    "                          label=data['target'],\n",
    "                          feature_name=[x for x in features if x not in ['resp','weight']])\n",
    "\n",
    "test_params['metric'] = 'auc'\n",
    "test_model = lgbm.cv(params = test_params,\n",
    "                        train_set = dtrain,\n",
    "                        num_boost_round=10000,\n",
    "                        nfold=5,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        metrics='auc',\n",
    "                        feature_name=[x for x in features if x not in ['resp','weight']],\n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trees_tune(X:pd.DataFrame, target:str, features:list, num_boost_rounds:int, params:dict, learning_rate:float = 0.01, num_iter:int = 10, train_sample:float = 0.7, verbose_eval:int = 0):\n",
    "    \n",
    "    cat_features_indices = [i for i in range(len(features)) if features[i] in cat_features]\n",
    "    params['learning_rate'] = learning_rate\n",
    "    best_iter_list = []\n",
    "        \n",
    "    for i in range(num_iter):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X[features], X[target], train_size = train_sample, shuffle = True, stratify = X[target])\n",
    "        if len(cat_features) == 0:\n",
    "            X_train_smote, y_train_smote = SMOTE(random_state=51).fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_train_smote, y_train_smote = SMOTENC(categorical_features=cat_features_indices, random_state=51).fit_resample(X_train, y_train)\n",
    "        \n",
    "        del(X_train, y_train)\n",
    "        gc.collect()\n",
    "        \n",
    "        dtrain = lgbm.Dataset(data = X_train_smote,\n",
    "                                label = y_train_smote,\n",
    "                                feature_name = features,\n",
    "                                categorical_feature = cat_features)\n",
    "        \n",
    "        dval = lgbm.Dataset(data = X_val,\n",
    "                            label = y_val,\n",
    "                            feature_name = features,\n",
    "                            categorical_feature = cat_features)\n",
    "        \n",
    "        del(X_train_smote, y_train_smote, X_val, y_val)\n",
    "        gc.collect()\n",
    "        \n",
    "        current_model = lgbm.train(params=params,\n",
    "                                    train_set=dtrain,\n",
    "                                    num_boost_round=num_boost_rounds,\n",
    "                                    valid_sets=[dval],\n",
    "                                    feature_name=features,\n",
    "                                    categorical_feature=cat_features,\n",
    "                                    early_stopping_rounds=1000,\n",
    "                                    verbose_eval=0)\n",
    "\n",
    "        best_iter_current = current_model.best_iteration\n",
    "        best_iter_list += [best_iter_current]\n",
    "        \n",
    "        print(current_model.best_score)\n",
    "\n",
    "    return int(np.nanmean(best_iter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['target', 'param', 'val'])\n",
    "\n",
    "params = {'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.25,\n",
    "            'is_unbalanced': False}\n",
    "    \n",
    "for target in targets:\n",
    "    train = train_all.copy()\n",
    "    augmented_vals = augmenting_vals(train, target, num = 20)\n",
    "    train = pd.concat([train, augmented_vals], axis = 0, ignore_index = True)\n",
    "    best_params = params_tune(X=train, target=target, features=features, cat_features=cat_features, num_boost_rounds=10000, params=params)\n",
    "    \n",
    "    features_to_keep = feature_selection_shap(X=train[features], y=train[target], params=best_params, features=features, cat_features=cat_features, sample=0.5, num_boost_rounds=10000, num_iter = 3, verbose_eval = 0)\n",
    "    cat_features_to_keep = [x for x in cat_features if x in features_to_keep]\n",
    "    \n",
    "    best_params['features'] = features_to_keep\n",
    "    best_params['cat_features'] = cat_features_to_keep\n",
    "    \n",
    "    num_trees = num_trees_tune(X=train, target=target, features=features_to_keep, cat_features=cat_features_to_keep, num_boost_rounds=50000, params=best_params, learning_rate = 0.01, num_iter = 10, train_sample = 0.7, verbose_eval = 0)\n",
    "    best_params['num_trees'] = int(1.1*num_trees)\n",
    "    \n",
    "    for key in best_params.keys():\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(data=[[target, key, best_params[key]]], columns=['target', 'param', 'val'])], axis=0, ignore_index=True)\n",
    "    print(results_df.tail(10))\n",
    "    print(target, ' completed. ', len(targets) - targets.index(target) - 1, ' to go...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
